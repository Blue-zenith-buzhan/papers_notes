# Enabling Execution Assurance of Federated Learning at Untrusted Participants



**联邦学习（Federated Learning, FL）**：作为一种保护隐私的机器学习框架，FL通过分布式训练任务到数据所有者并聚合他们的模型更新来获得联合准确的模型。然而，由于FL失去了对训练过程的直接控制，它面临新的安全问题，特别是如何确保参与者按照预期执行训练任务。



**TrustFL方案**：提出了一种结合可信执行环境（TEE）和GPU的实用方案TrustFL，以高效且高置信度地确保参与者的训练执行。

- **TEE与GPU结合**：使用TEE随机检查一小部分训练过程以提供可调级别的保证，而所有计算都在共置的更快但不安全的处理器（如GPU）上执行以提高效率。直接在TEE中执行所有DNN训练过程会导致性能大幅下降。TrustFL通过仅随机验证一小部分训练轮次来解决此问题。

- **承诺与验证方法**：设计了一种基于承诺的方法，通过特定的数据选择来防止各种作弊行为，如仅处理TEE请求的计算或上传旧结果。

- **动态且确定性的数据选择**：利用动态输入数据使任何两轮的训练结果不同，从而防止使用旧结果的重放攻击。

  

## 方案概述

- **任务招标**：任务所有者发布一个需要高精度DNN模型的任务，并附带奖励。
- **注册**：感兴趣的参与者注册并提供必要信息，包括数据描述和标识符。参与者还需生成并验证程序设置的正确性。
- **选择**：在每个训练周期，服务器随机选择一部分参与者进行训练。
- **报告**：参与者使用GPU进行本地训练，并使用TEE生成训练执行的证明。
- **验证与聚合**：服务器验证证明，分发奖励，并聚合验证通过的模型更新以生成新的全局模型。

遵循“commit-and-prove”范式，设计了基线方案。在TEE外部的程序`Prog_o`存储每轮训练结束时所有模型参数θs的哈希摘要，并在所有训练轮次完成后向TEE发送一个提交消息。

- **训练轮次提交**：`Prog_o`在每个训练轮次结束时计算模型参数的哈希摘要，并在所有轮次完成后将这些哈希摘要和最终的模型参数发送给TEE。
- **随机抽样验证**：TEE随机选择一定数量的训练轮次，要求参与者将这些轮次的详细数据（如模型参数、梯度等）重新加载到TEE内进行验证。参与者需要证明这些轮次的计算结果是正确的，并生成相应的证明。



## 攻击与解决

- 通过“承诺与证明”设计，确保参与者在暴露采样决策前提交承诺消息。
- 通过动态且确定性的数据选择方法，确保训练结果的新鲜性，防止重放攻击。
- 使用防篡改标识符来防止Sybil攻击，确保奖励分配与训练努力成正比。
