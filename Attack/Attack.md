# 一些攻击方法



## 拜占庭攻击：

1. **数据投毒**：恶意参与方可以在其本地数据集中注入有毒数据（即被精心设计以误导模型训练的数据），然后在模型训练过程中将这些数据发送给中央服务器。由于联邦学习是基于分布式数据训练的，这些有毒数据可能会被模型学习并导致模型性能下降或产生偏差。
2. **模型更新篡改**：恶意参与方可以在其本地模型更新中添加噪声或错误信息，然后将这些篡改后的更新发送给中央服务器。由于联邦学习通常依赖于多个参与方的模型更新来优化全局模型，这些篡改后的更新可能会破坏全局模型的收敛性和准确性。
3. **身份伪造**：恶意参与方可能伪造自己的身份并冒充其他合法参与方参与模型训练。通过控制多个伪造的参与方，他们可以发送大量错误或不一致的模型更新来干扰全局模型的训练过程。



## 后门攻击：

1. **数据投毒（后门植入）**：攻击者通过在训练数据集中植入带有特定触发器的数据样本，使模型学习到这些触发器与特定输出之间的关联。当模型遇到带有触发器的输入时，就会输出攻击者预设的结果。
2. **模型修改**：攻击者直接修改AI模型的参数或结构，以在模型中植入后门。这种方式需要攻击者具有对模型内部结构的深入了解，但一旦成功，其隐蔽性和针对性通常更强。



## 推理攻击

1. **模型逆向攻击**：旨在通过模型的输出或行为来逆向推导出模型的参数或结构。这种攻击通常依赖于对模型内部机制的深入理解，以及大量的输入输出对样本。
2. **属性推理攻击（PIA, Property Inference Attacks）**：关注于推理数据集的整体统计特征或属性，而非具体的数据记录。攻击者可能利用模型的输出或行为来推断训练数据集的某些敏感属性，如数据分布、类别比例等。
3. **成员推理攻击（MIAs, Membership Inference Attacks）**：旨在推断一个特定的数据记录是否被用于训练目标机器学习模型。这种攻击对于敏感且稀缺的数据集尤为危险，因为它可能泄露数据持有者的隐私。
4. **模型推理攻击**：更广泛地指利用模型的行为或输出来推断出模型本身或训练数据的信息。这种攻击可能涉及多种技术和方法，如梯度匹配、数据重构等。
